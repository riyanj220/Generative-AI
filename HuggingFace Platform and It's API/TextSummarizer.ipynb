{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"kw-fnOYjvc_u"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rEldQWTWvc_x"},"outputs":[],"source":["!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jf4rK0oevc_y"},"outputs":[],"source":["!pip install --upgrade accelerate\n","!pip uninstall -y transformers accelerate\n","!pip install transformers accelerate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mvcVPR_Yvc_z"},"outputs":[],"source":["!pip install evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WGqTlsR4vc_0"},"outputs":[],"source":["import evaluate\n","from transformers import pipeline,set_seed\n","from datasets import load_dataset, load_from_disk\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","# from evaluate import load_metric\n","\n","from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n","\n","import nltk\n","from nltk.tokenize import sent_tokenize\n","\n","from tqdm import tqdm\n","import torch\n","\n","nltk.download(\"punkt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vhkJn2OGvc_1","outputId":"78239bb0-2ea2-45ad-ac7c-83b55362775f"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'cuda'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QtEXriBDvc_3"},"outputs":[],"source":["model_ckpt = \"google/pegasus-cnn_dailymail\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2tzBlzBwvc_3"},"outputs":[],"source":["model_progress = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b92Lqnfyvc_4"},"outputs":[],"source":["dataset_samsum = load_dataset(\"knkarthick/samsum\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DR3DYnWnvc_7"},"outputs":[],"source":["dataset_samsum"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kNqrF8R6vc_8","outputId":"b28cbe2e-0aa0-4c51-d1cd-552edf797a1c"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"Amanda: I baked  cookies. Do you want some?\\nJerry: Sure!\\nAmanda: I'll bring you tomorrow :-)\""]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["dataset_samsum['train']['dialogue'][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DW1pZLRkvc_9","outputId":"c47ce799-d0f3-4a5c-fcd0-731d8451b615"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Amanda baked cookies and will bring Jerry some tomorrow.'"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["dataset_samsum['train']['summary'][0]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dwhz7n1hvc__"},"outputs":[],"source":["split_lengths = [len(dataset_samsum[split]) for split in dataset_samsum]\n","\n","print(f\"Split lengths: {split_lengths}\")\n","print(f\"Features: {dataset_samsum['train'].column_names}\")\n","print(\"\\nDialogue: \")\n","\n","print(dataset_samsum[\"test\"][1][\"dialogue\"])\n","\n","print(\"\\nSummary: \")\n","print(dataset_samsum[\"test\"][1][\"summary\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ysmt_hg3vdAA"},"outputs":[],"source":["# Step 1: Filter out non-string or empty dialogues/summaries\n","def filter_bad_examples(example):\n","    return isinstance(example['dialogue'], str) and isinstance(example['summary'], str) and \\\n","           len(example['dialogue'].strip()) > 0 and len(example['summary'].strip()) > 0\n","\n","dataset_samsum_clean = dataset_samsum.filter(filter_bad_examples)\n","\n","# Step 2: Define tokenization function (no need to filter inside)\n","def convert_examples_to_features(example_batch):\n","    # Tokenize inputs\n","    model_inputs = tokenizer(example_batch['dialogue'], max_length=1024, truncation=True, padding='max_length')\n","\n","    # Tokenize summaries as labels\n","    labels = tokenizer(text_target=example_batch['summary'], max_length=128, truncation=True, padding='max_length')\n","    model_inputs['labels'] = labels['input_ids']\n","\n","    return model_inputs\n","\n","# Step 3: Map over the cleaned dataset\n","dataset_samsum_pt = dataset_samsum_clean.map(convert_examples_to_features, batched=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MvniNiMrvdAB","outputId":"2af22fec-dd36-4eda-b6a8-3136acf8016e"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"Amanda: I baked  cookies. Do you want some?\\nJerry: Sure!\\nAmanda: I'll bring you tomorrow :-)\""]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["dataset_samsum_pt['train']['dialogue'][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2lZbSxULvdAC","outputId":"07e4e4fb-e05a-4161-a5fa-76d7633f1061"},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n","    num_rows: 14731\n","})"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["dataset_samsum_pt['train']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"no5YPsbcvdAD"},"outputs":[],"source":["dataset_samsum_pt['train']['attention_mask'][1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LvWZdT7lvdAE"},"outputs":[],"source":["# Training\n","\n","from transformers import DataCollatorForSeq2Seq\n","\n","seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_progress)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0fGD6X1JvdAF","outputId":"87a9a7ac-da63-4c31-e382-aed6e73ee078"},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/transformers/__init__.py\n"]}],"source":["import transformers\n","print(transformers.__file__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"svECe91cvdAG"},"outputs":[],"source":["from transformers import TrainingArguments, Trainer\n","import transformers\n","training_args = TrainingArguments(\n","    output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500,\n","    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n","    weight_decay=0.01, logging_steps=10,\n","    eval_strategy='steps', eval_steps=500, save_steps=int(1e6),\n","    gradient_accumulation_steps=16\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QkjibJnwvdAH","outputId":"677b297c-bd05-4516-fe57-ca30a549c5f7"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipython-input-3484580808.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer (model = model_progress, args = training_args ,\n"]}],"source":["trainer = Trainer (model = model_progress, args = training_args ,\n","                   tokenizer = tokenizer , data_collator = seq2seq_data_collator,\n","                   train_dataset = dataset_samsum_pt['test'],\n","                   eval_dataset=dataset_samsum_pt['validation'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E8gJnHtQvdAI","outputId":"7b9f6223-ae9e-4baf-bb44-322421b14aa1"},"outputs":[{"data":{"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n","wandb: Paste an API key from your profile and hit enter:"]},{"name":"stdout","output_type":"stream","text":[" 路路路路路路路路路路\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mriyanjamil220\u001b[0m (\u001b[33mriyanjamil220-rj\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.21.1"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20250816_064838-1jp0ndez</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/riyanjamil220-rj/huggingface/runs/1jp0ndez' target=\"_blank\">gallant-tree-5</a></strong> to <a href='https://wandb.ai/riyanjamil220-rj/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/riyanjamil220-rj/huggingface' target=\"_blank\">https://wandb.ai/riyanjamil220-rj/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/riyanjamil220-rj/huggingface/runs/1jp0ndez' target=\"_blank\">https://wandb.ai/riyanjamil220-rj/huggingface/runs/1jp0ndez</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='52' max='52' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [52/52 13:22, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3917: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n"]},{"data":{"text/plain":["TrainOutput(global_step=52, training_loss=10.502341490525465, metrics={'train_runtime': 950.4835, 'train_samples_per_second': 0.862, 'train_steps_per_second': 0.055, 'total_flos': 2366471355236352.0, 'train_loss': 10.502341490525465, 'epoch': 1.0})"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5TF05NFvdAK"},"outputs":[],"source":["# Evaluation\n","\n","def generate_batch_sized_chunks(list_of_elements, batch_size):\n","    \"\"\"split the dataset into smaller batches that we can process simultaneously\n","    Yield successive batch-sized chunks from list_of_elements\"\"\"\n","\n","    for i in range(0, len(list_of_elements), batch_size):\n","        yield list_of_elements[i : i + batch_size]\n","\n","def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n","                               batch_size=16, device=device,\n","                               column_text=\"article\",\n","                               column_summary=\"highlights\"):\n","    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n","    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n","\n","    for article_batch, target_batch in tqdm(\n","        zip(article_batches, target_batches), total=len(article_batches)):\n","\n","        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n","                        padding=\"max_length\", return_tensors=\"pt\")\n","\n","        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n","                         attention_mask=inputs[\"attention_mask\"].to(device),\n","                         length_penalty=0.8, num_beams=8, max_length=128)\n","\n","        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n","\n","        # Finally we decode the generated texts,\n","        # Replace the token and add the decoded texts with the reference to the metric\n","        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n","                                clean_up_tokenization_spaces=True)\n","               for s in summaries]\n","\n","        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n","\n","        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n","\n","    #  Finally compute and return the ROUGE scores.\n","    score = metric.compute()\n","    return score\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oe3ZCOJlvdAO","outputId":"b328dd16-cb7e-40b9-a958-a5e9d9afe845","colab":{"referenced_widgets":["c8f04d9ddde04073a6c6b7c88b0aedfe"]}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c8f04d9ddde04073a6c6b7c88b0aedfe","version_major":2,"version_minor":0},"text/plain":["Downloading builder script: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n","rouge_metric = evaluate.load('rouge')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tuH67duwvdAP","outputId":"6cd489dd-1bef-4659-84e7-44b8d44269ca"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|| 5/5 [00:30<00:00,  6.01s/it]\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"pd\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"rouge1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.018969391570119905,\n        \"max\": 0.018969391570119905,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.018969391570119905\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rouge2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rougeL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.018464121698793058,\n        \"max\": 0.018464121698793058,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.018464121698793058\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rougeLsum\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.01872128679529215,\n        \"max\": 0.01872128679529215,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.01872128679529215\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe"},"text/html":["\n","  <div id=\"df-9274e36a-dd17-40f4-ba2e-d7c22c4e3aa9\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>rouge1</th>\n","      <th>rouge2</th>\n","      <th>rougeL</th>\n","      <th>rougeLsum</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>pegasus</th>\n","      <td>0.018969</td>\n","      <td>0.0</td>\n","      <td>0.018464</td>\n","      <td>0.018721</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9274e36a-dd17-40f4-ba2e-d7c22c4e3aa9')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-9274e36a-dd17-40f4-ba2e-d7c22c4e3aa9 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-9274e36a-dd17-40f4-ba2e-d7c22c4e3aa9');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"text/plain":["           rouge1  rouge2    rougeL  rougeLsum\n","pegasus  0.018969     0.0  0.018464   0.018721"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["score = calculate_metric_on_test_ds(\n","    dataset_samsum['test'][0:10], rouge_metric, trainer.model, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary')\n","\n","rouge_dict = {rn: score[rn] for rn in rouge_names}\n","\n","pd.DataFrame(rouge_dict, index = [f'pegasus'] )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m33PMBG1vdAR"},"outputs":[],"source":["## Save the model\n","model_progress.save_pretrained(\"pegasus-samsum-model\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yWk8NwzWvdAS","outputId":"54308009-c2bc-47f7-b55b-5edf542fa049"},"outputs":[{"data":{"text/plain":["('tokenizer/tokenizer_config.json',\n"," 'tokenizer/special_tokens_map.json',\n"," 'tokenizer/spiece.model',\n"," 'tokenizer/added_tokens.json',\n"," 'tokenizer/tokenizer.json')"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["## Save the tokenizer\n","\n","tokenizer.save_pretrained(\"tokenizer\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pS7NU-6PvdAT"},"outputs":[],"source":["# load\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"/content/tokenizer\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E1zt1LCgvdAU","outputId":"87c3bfb2-4926-4fb0-9fcb-6fadc68c210c"},"outputs":[{"name":"stderr","output_type":"stream","text":["Device set to use cuda:0\n","Your max_length is set to 128, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n"]},{"name":"stdout","output_type":"stream","text":["Dialogue:\n","Hannah: Hey, do you have Betty's number?\n","Amanda: Lemme check\n","Hannah: <file_gif>\n","Amanda: Sorry, can't find it.\n","Amanda: Ask Larry\n","Amanda: He called her last time we were at the park together\n","Hannah: I don't know him well\n","Hannah: <file_gif>\n","Amanda: Don't be shy, he's very nice\n","Hannah: If you say so..\n","Hannah: I'd rather you texted him\n","Amanda: Just text him \n","Hannah: Urgh.. Alright\n","Hannah: Bye\n","Amanda: Bye bye\n","\n","Reference Summary:\n","Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n","\n","Model Summary:\n","Amanda: Ask Larry Amanda: He called her last time we were at the park together .<n>Hannah: I'd rather you texted him .<n>Amanda: Just text him .\n"]}],"source":["# Prediction\n","gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\n","\n","sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n","\n","reference = dataset_samsum[\"test\"][0][\"summary\"]\n","\n","pipe = pipeline(\"summarization\", model=\"pegasus-samsum-model\",tokenizer=tokenizer)\n","\n","print(\"Dialogue:\")\n","print(sample_text)\n","\n","\n","print(\"\\nReference Summary:\")\n","print(reference)\n","\n","\n","print(\"\\nModel Summary:\")\n","print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u_SHGVndvdAV"},"outputs":[],"source":[]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}