{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title # **0) Install deps (2025-safe)**\n",
    "!pip install -q --upgrade llama-index-core llama-index \\\n",
    "         llama-index-vector-stores-faiss faiss-cpu pypdf \\\n",
    "         sentence-transformers llama-index-embeddings-huggingface \\\n",
    "         llama-index-llms-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title # **1) Imports**\n",
    "\n",
    "# Core LlamaIndex\n",
    "from llama_index.core import (\n",
    "    Settings,\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    get_response_synthesizer,\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.storage import StorageContext\n",
    "from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler\n",
    "\n",
    "# Hugging Face LLM + Embeddings\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# FAISS vector store\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title # **2) Paths & runtime knobs**\n",
    "DATA_DIR = \"/content/data\"             # your PDFs\n",
    "PERSIST_DIR = \"/content/faiss_store\"   # FAISS index storage\n",
    "CHUNK_SIZE = 1024\n",
    "CHUNK_OVERLAP = 100\n",
    "TOP_K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d511864242041f2ad52a30666efa8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad5bf04d7c445cbbe0e02df45493d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb56580695647a1a25e8d84e512cb4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b3b471dba54c68bc81005ec6e18383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:llama_index.llms.huggingface.base:Supplied context_window 3900 is greater than the model's max input size 2048. Disable this warning by setting a lower context_window.\n"
     ]
    }
   ],
   "source": [
    "# @title # **3) Configure LLM + Embeddings + Settings**\n",
    "from transformers import AutoTokenizer\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name=\"facebook/opt-350m\",\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    ")\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "dbg = LlamaDebugHandler(print_trace_on_end=True)\n",
    "\n",
    "\n",
    "Settings.callback_manager = CallbackManager([dbg])\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 176 document(s) from /content/data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7de61a753f4949aff6c48dfc65969f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d31fd3075e504bc49e3d93d25ca41e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "    |_CBEventType.EMBEDDING -> 2.360265 seconds\n",
      "    |_CBEventType.EMBEDDING -> 2.593183 seconds\n",
      "    |_CBEventType.EMBEDDING -> 1.219504 seconds\n",
      "    |_CBEventType.EMBEDDING -> 3.492537 seconds\n",
      "    |_CBEventType.EMBEDDING -> 1.920979 seconds\n",
      "    |_CBEventType.EMBEDDING -> 1.601589 seconds\n",
      "    |_CBEventType.EMBEDDING -> 1.43399 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.734126 seconds\n",
      "    |_CBEventType.EMBEDDING -> 1.836512 seconds\n",
      "    |_CBEventType.EMBEDDING -> 2.937422 seconds\n",
      "    |_CBEventType.EMBEDDING -> 3.397503 seconds\n",
      "    |_CBEventType.EMBEDDING -> 2.463823 seconds\n",
      "    |_CBEventType.EMBEDDING -> 0.870543 seconds\n",
      "    |_CBEventType.EMBEDDING -> 1.563879 seconds\n",
      "    |_CBEventType.EMBEDDING -> 4.045609 seconds\n",
      "    |_CBEventType.EMBEDDING -> 3.57953 seconds\n",
      "    |_CBEventType.EMBEDDING -> 2.543255 seconds\n",
      "    |_CBEventType.EMBEDDING -> 1.390232 seconds\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "# @title # **4) Create FAISS index + Build LlamaIndex**\n",
    "# initialize FAISS index\n",
    "embed_dim = len(embed_model.get_text_embedding(\"test\"))\n",
    "faiss_index = faiss.IndexFlatL2(embed_dim)\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# load PDFs\n",
    "reader = SimpleDirectoryReader(input_dir=DATA_DIR, recursive=True)\n",
    "documents = reader.load_data()\n",
    "print(f\"Loaded {len(documents)} document(s) from {DATA_DIR}\")\n",
    "\n",
    "# build vector index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    show_progress=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title # **5) Query Engine**\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=TOP_K,\n",
    "    response_mode=\"compact\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: Give me a concise overview of the PDFs in 3 bullets.\n",
      "**********\n",
      "Trace: query\n",
      "    |_CBEventType.QUERY -> 53.815125 seconds\n",
      "      |_CBEventType.RETRIEVE -> 0.030008 seconds\n",
      "        |_CBEventType.EMBEDDING -> 0.027951 seconds\n",
      "      |_CBEventType.SYNTHESIZE -> 53.784302 seconds\n",
      "        |_CBEventType.TEMPLATING -> 2.5e-05 seconds\n",
      "        |_CBEventType.LLM -> 53.780509 seconds\n",
      "        |_CBEventType.LLM -> 53.780003 seconds\n",
      "**********\n",
      "\n",
      "A:                                                                                                                                                                                                                                                                 \n",
      "\n",
      "Sources:\n",
      "1. Data structures (Binary Search Tree).pdf | score=1.308\n",
      "2. Data structures (Array 1D).pdf | score=1.319\n",
      "3. Data structures (Array 1D).pdf | score=1.326\n",
      "\n",
      "Q: What are the main topics discussed?\n",
      "**********\n",
      "Trace: query\n",
      "    |_CBEventType.QUERY -> 69.571506 seconds\n",
      "      |_CBEventType.RETRIEVE -> 0.025046 seconds\n",
      "        |_CBEventType.EMBEDDING -> 0.02318 seconds\n",
      "      |_CBEventType.SYNTHESIZE -> 69.545577 seconds\n",
      "        |_CBEventType.TEMPLATING -> 2.4e-05 seconds\n",
      "        |_CBEventType.LLM -> 69.541309 seconds\n",
      "        |_CBEventType.LLM -> 69.540885 seconds\n",
      "**********\n",
      "\n",
      "A: ------------------------\n",
      "\n",
      "page_label: 6\n",
      "file_path: /content/data/Data structures (Graph).pdf\n",
      "\n",
      "What is Fractional Knapsack Problem?\n",
      "There are two version of fractional knapsack problem\n",
      "1. 0-1 fractional knapsack problem\n",
      "Items are indivisible (either take an item or not)\n",
      "Can be solved with dynamic programming\n",
      "2. Fractional Knapsack problem\n",
      "Items are divisible (can take any fraction of an item)\n",
      "It can be solved in greedy ,method\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: What are the main topics discussed?\n",
      "Answer: ------------------------\n",
      "\n",
      "page_label: 7\n",
      "file_path: /content/data/Data structures (Graph).pdf\n",
      "\n",
      "What is Fractional Knapsack Problem?\n",
      "There are two version of fractional knapsack problem\n",
      "1. 0-1 fractional knapsack problem\n",
      "Items are indivisible (either take an item or not)\n",
      "Can be solved with dynamic programming\n",
      "2. Fractional Knapsack problem\n",
      "Items are divisible (can take any fraction of an item)\n",
      "It can be solved in\n",
      "\n",
      "Sources:\n",
      "1. Data structures (Graph).pdf | score=1.545\n",
      "2. Data structures (Graph).pdf | score=1.644\n",
      "3. Data structures (Binary Trees).pdf | score=1.700\n"
     ]
    }
   ],
   "source": [
    "# @title # **6) Try some questions**\n",
    "\n",
    "questions = [\n",
    "    \"Give me a concise overview of the PDFs in 3 bullets.\",\n",
    "    \"What are the main topics discussed?\",\n",
    "]\n",
    "\n",
    "import asyncio\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    resp = await query_engine.aquery(q)\n",
    "    print(\"\\nA:\", str(resp))\n",
    "\n",
    "    print(\"\\nSources:\")\n",
    "    if resp.source_nodes:\n",
    "        for i, node in enumerate(resp.source_nodes[:3], start=1):\n",
    "            print(f\"{i}. {node.metadata.get('file_name', 'unknown')} | score={node.score:.3f}\")\n",
    "    else:\n",
    "        print(\"No sources found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title # **7) Save FAISS index**\n",
    "\n",
    "# save the raw FAISS index\n",
    "faiss.write_index(faiss_index, f\"{PERSIST_DIR}/faiss.index\")\n",
    "\n",
    "# save storage context metadata\n",
    "storage_context.persist(persist_dir=PERSIST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from /content/faiss_store/docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from /content/faiss_store/index_store.json.\n",
      "**********\n",
      "Trace: index_construction\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "# @title # **8) Reload FAISS index without embedding again**\n",
    "\n",
    "from llama_index.core import load_index_from_storage\n",
    "\n",
    "# reload FAISS\n",
    "faiss_index = faiss.read_index(f\"{PERSIST_DIR}/faiss.index\")\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store,\n",
    "    persist_dir=PERSIST_DIR,\n",
    ")\n",
    "\n",
    "index = load_index_from_storage(storage_context)\n",
    "query_engine = index.as_query_engine(similarity_top_k=TOP_K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
