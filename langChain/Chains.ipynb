{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xRL2o-2BnMO"
   },
   "source": [
    "# **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq langchain\n",
    "!pip install -qq langchain-community\n",
    "!pip install -qq langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_API_KEY'] = GEMINI_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3,google_api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbtmaWyPBw_Q"
   },
   "source": [
    "# **Single Chain (Sequential)**\n",
    "\n",
    "The simplest chain: Prompt → LLM → Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain simplifies the development of LLM-powered applications by providing tools for managing prompts, chaining operations, implementing memory, and creating autonomous agents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Summarize this text in one sentence:\\n\\n{text}\"\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Compose pipeline using |\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "result = chain.invoke({\"text\": \"LangChain helps developers build apps powered by LLMs with prompts, chains, memory, and agents.\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYVb8QVID1cG"
   },
   "source": [
    "# **Parallel Chains (RunnableParallel)**\n",
    "\n",
    "Run multiple chains (or models) at the same time → returns a dict of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary': 'LangChain simplifies LLM interaction via modular prompts, chains, memory, and agents.', 'keywords': 'LangChain, LLMs, Agents'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# Two different prompt chains\n",
    "summary_prompt = ChatPromptTemplate.from_template(\"Summarize in 10 words:\\n\\n{text}\")\n",
    "keywords_prompt = ChatPromptTemplate.from_template(\"Extract 3 keywords:\\n\\n{text}\")\n",
    "\n",
    "summary_chain = summary_prompt | llm | parser\n",
    "keywords_chain = keywords_prompt | llm | parser\n",
    "\n",
    "# Run them in parallel\n",
    "parallel_chain = RunnableParallel(\n",
    "    summary=summary_chain,\n",
    "    keywords=keywords_chain\n",
    ")\n",
    "\n",
    "result = parallel_chain.invoke({\n",
    "    \"text\": \"LangChain simplifies working with LLMs by providing abstractions for prompts, chains, memory, and agents.\"\n",
    "})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IC72UtSZEEjD"
   },
   "source": [
    "# **Conditional Chains (RunnableBranch)**\n",
    "\n",
    "Choose which chain to run depending on input conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "# Prompt for short vs long text\n",
    "short_prompt = ChatPromptTemplate.from_template(\"Summarize briefly: {text}\")\n",
    "long_prompt = ChatPromptTemplate.from_template(\"Give a detailed explanation of: {text}\")\n",
    "\n",
    "short_chain = short_prompt | llm | parser\n",
    "long_chain = long_prompt | llm | parser\n",
    "\n",
    "# Branch logic\n",
    "conditional_chain = RunnableBranch(\n",
    "    (lambda x: len(x[\"text\"].split()) < 10, short_chain),\n",
    "    (long_chain)  # default case\n",
    ")\n",
    "\n",
    "print(conditional_chain.invoke({\"text\": \"AI changes everything.\"}))\n",
    "print(conditional_chain.invoke({\"text\": \"LangChain provides abstractions like prompts, chains, agents, and memory to make building LLM apps much easier.\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
