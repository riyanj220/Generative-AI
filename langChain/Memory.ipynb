{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJPRKxevzwAR"
   },
   "source": [
    "# **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq langchain langchain-core langchain-google-genai langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_API_KEY'] = GEMINI_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3,google_api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.memory import ChatMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMX6D7QJzr5Q"
   },
   "source": [
    "# **Conversation buffer memory**\n",
    "keeps all conversation history (big context → expensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.3.27\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /usr/local/lib/python3.12/dist-packages\n",
      "Requires: langchain-core, langchain-text-splitters, langsmith, pydantic, PyYAML, requests, SQLAlchemy\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Ali! It's nice to meet you.  How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# 1. Define where to keep memory (per-session store)\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> ChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# 2. Prompt with history slot\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a friendly assistant.\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# 3. Base chain\n",
    "base_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 4. Add memory wrapper\n",
    "chain = RunnableWithMessageHistory(\n",
    "    base_chain,\n",
    "    get_session_history,                # function that manages histories\n",
    "    history_messages_key=\"history\",     # must match MessagesPlaceholder\n",
    "    input_messages_key=\"input\"          # user input field\n",
    ")\n",
    "\n",
    "# 5. Run conversation (provide session_id each time)\n",
    "print(chain.invoke(\n",
    "    {\"input\": \"hello, my name is ali.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user-123\"}}\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Ali.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\n",
    "    {\"input\": \"what’s my name?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"user-123\"}}\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQjWqvFG4s26"
   },
   "source": [
    "# **Conversation buffer window memory**\n",
    "only last n exchanges (cheaper, but forgets old info)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Ali! It's nice to meet you. How can I help you today?\n",
      "That's great, Ali! Karachi is a vibrant city.  Is there anything specific you'd like to talk about regarding Karachi, or anything else I can help you with?\n",
      "That's fantastic, Ali! Computer Science is a fascinating and rapidly evolving field.  What aspects of computer science are you most interested in?  Are you currently working on any interesting projects?  I'd love to hear more.\n",
      "I don't have access to personal information about you, Ali, unless you explicitly provide it to me.  My purpose is to help and provide information, but I prioritize your privacy.  I only know what you've told me in our current conversation.\n"
     ]
    }
   ],
   "source": [
    "# 1. Store with window control\n",
    "store = {}\n",
    "WINDOW_SIZE = 3  # keep last 3 messages\n",
    "\n",
    "def get_session_history(session_id: str) -> ChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    history = store[session_id]\n",
    "    # trim history to last k messages\n",
    "    if len(history.messages) > WINDOW_SIZE:\n",
    "        history.messages = history.messages[-WINDOW_SIZE:]\n",
    "    return history\n",
    "\n",
    "# 2. Prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a friendly assistant.\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# 3. Base chain\n",
    "base_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 4. Memory wrapper\n",
    "chain = RunnableWithMessageHistory(\n",
    "    base_chain,\n",
    "    get_session_history,\n",
    "    history_messages_key=\"history\",\n",
    "    input_messages_key=\"input\"\n",
    ")\n",
    "\n",
    "# 5. Run\n",
    "print(chain.invoke({\"input\": \"Hello, I am Ali.\"}, config={\"configurable\": {\"session_id\": \"u1\"}}))\n",
    "print(chain.invoke({\"input\": \"I live in Karachi.\"}, config={\"configurable\": {\"session_id\": \"u1\"}}))\n",
    "print(chain.invoke({\"input\": \"I study computer science.\"}, config={\"configurable\": {\"session_id\": \"u1\"}}))\n",
    "print(chain.invoke({\"input\": \"What’s my name and where do I live?\"}, config={\"configurable\": {\"session_id\": \"u1\"}}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmP9lxdz544f"
   },
   "source": [
    "# **Summarizer memory**\n",
    "keeps a running summary instead of raw history (scales better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3651643799.py:8: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  store[session_id] = ConversationSummaryBufferMemory(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Ali! It's nice to meet you. How can I help you today?\n",
      "That's great, Ali! Karachi is a vibrant city.  Is there anything specific you'd like to talk about regarding Karachi, or anything else I can help you with?\n",
      "That's fantastic, Ali! Computer Science is a fascinating and rapidly evolving field.  Are you working on any interesting projects right now?  Or perhaps you have questions about a particular area of computer science?  I'd be happy to chat if you'd like.\n",
      "Your name is Ali, and you told me you live in Karachi and you study computer science.\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "# 1. Memory store\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ConversationSummaryBufferMemory(\n",
    "            llm=llm,\n",
    "            return_messages=True,\n",
    "            max_token_limit=200\n",
    "        )\n",
    "    return store[session_id].chat_memory  # ✅ exposes .messages\n",
    "\n",
    "# 2. Prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a friendly assistant.\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# 3. Base chain\n",
    "base_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 4. Memory wrapper\n",
    "chain = RunnableWithMessageHistory(\n",
    "    base_chain,\n",
    "    get_session_history,\n",
    "    history_messages_key=\"history\",\n",
    "    input_messages_key=\"input\"\n",
    ")\n",
    "\n",
    "# 5. Run\n",
    "print(chain.invoke({\"input\": \"Hello, I am Ali.\"}, config={\"configurable\": {\"session_id\": \"u2\"}}))\n",
    "print(chain.invoke({\"input\": \"I live in Karachi.\"}, config={\"configurable\": {\"session_id\": \"u2\"}}))\n",
    "print(chain.invoke({\"input\": \"I study computer science.\"}, config={\"configurable\": {\"session_id\": \"u2\"}}))\n",
    "print(chain.invoke({\"input\": \"What’s my name and what did I tell you earlier?\"}, config={\"configurable\": {\"session_id\": \"u2\"}}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
