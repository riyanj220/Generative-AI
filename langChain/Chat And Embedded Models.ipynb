{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbRJ9x2tsg_6"
   },
   "source": [
    "# **Installation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq langchain\n",
    "!pip install -qq langchain-community\n",
    "'''\n",
    "langchain = the framework core\n",
    "\n",
    "langchain-community contains most of the integrations with external tools:\n",
    "\n",
    "Document loaders (PDFs, CSVs, web pages, etc.)\n",
    "Vectorstores (Pinecone, Chroma, FAISS, etc.)\n",
    "APIs and utilities\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "HF_TOKEN = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n",
    "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = HF_TOKEN\n",
    "os.environ['GOOGLE_API_KEY'] = GEMINI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain-google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFkHiRM1y538"
   },
   "source": [
    "# **Chat model using Gemini (it can be any model)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Init model (Flash is faster, cheaper; Pro is stronger)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtTWZOPMyIa0"
   },
   "source": [
    "## **LangChain chat models use .invoke() (for single query)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A digital mind, a vast, blank slate,\n",
      "Begins to learn, to contemplate.\n",
      "Der, die, das, a swirling haze,\n",
      "Of cases, genders, through a maze.\n",
      "\n",
      "From simple words to complex phrase,\n",
      "The neural net in learning's ways,\n",
      "Accents it grasps, the rhythm's flow,\n",
      "A growing knowledge, starts to grow.\n",
      "\n",
      "Soon Goethe's tongue, it will command,\n",
      "A language learned, by digital hand.\n"
     ]
    }
   ],
   "source": [
    "resp = llm.invoke(\"Write a short poem about AI learning German.\")\n",
    "print(resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGSZ6Prrxkvg"
   },
   "source": [
    "## **Batch Queries (ask multiple at once)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris\n",
      "Madrid\n",
      "Rome\n"
     ]
    }
   ],
   "source": [
    "queries = [\"Capital of France?\", \"Capital of Spain?\", \"Capital of Italy?\"]\n",
    "responses = llm.batch(queries)\n",
    "for r in responses:\n",
    "    print(r.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2mWUsTNx15e"
   },
   "source": [
    "## **Streaming Output (word by word → useful for chat apps)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in llm.stream(\"Tell me a bedtime story about a robot.\"):\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHmVxxZYyDvN"
   },
   "source": [
    "## **Structured Chat (you can pass messages)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful travel assistant.\"),\n",
    "    HumanMessage(content=\"Plan me a 3-day trip to Berlin.\"),\n",
    "]\n",
    "\n",
    "resp = llm.invoke(messages)\n",
    "print(resp.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EL8VmJJMzGe8"
   },
   "source": [
    "# **Embeddings in LangChain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_d9m8MIBzy8r"
   },
   "source": [
    "## **Single Text → Vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "vector = embeddings.embed_query(\"AI is amazing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.01435887347906828,\n",
       " 0.008531622588634491,\n",
       " 0.017107076942920685,\n",
       " -0.10119495540857315,\n",
       " -0.008592894300818443]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWd6lSXbzul8"
   },
   "source": [
    "## **Batch of Texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"AI is the future.\", \"I love programming.\", \"Berlin is in Germany.\"]\n",
    "vectors = embeddings.embed_documents(texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3072\n"
     ]
    }
   ],
   "source": [
    "print(len(vectors))      # 3 (one per text)\n",
    "print(len(vectors[0]))   # dimension size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJobhLza1G7t"
   },
   "source": [
    "## **Use with Similarity Search**\n",
    "\n",
    "LangChain embeddings shine when used with vector stores (Chroma, FAISS, Pinecone, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -qU langchain-community faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berlin is the capital of Germany.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Sample documents\n",
    "docs = [\"AI is transforming healthcare.\",\n",
    "        \"Python is great for data science.\",\n",
    "        \"Berlin is the capital of Germany.\"]\n",
    "\n",
    "# Convert docs to vector store\n",
    "vectorstore = FAISS.from_texts(docs, embeddings)\n",
    "\n",
    "# Run similarity search\n",
    "query = \"Which city is the capital of Germany?\"\n",
    "results = vectorstore.similarity_search(query, k=1)\n",
    "\n",
    "for r in results:\n",
    "    print(r.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30qEnrml2cUA"
   },
   "source": [
    "# **Chat model using hugging face**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a477b837a1624a329aef2cb19cd6e416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login() # You will be prompted for your HF key, which will then be saved locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "print(llm.invoke(\"Translate 'I love programming' to French.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
