{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eir90w2z07YN"
   },
   "source": [
    "# **Basic Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq langchain\n",
    "!pip install -qq langchain-community\n",
    "!pip install -qq langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_API_KEY'] = GEMINI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3,google_api_key=GEMINI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgKoebaZgrWU"
   },
   "source": [
    "# **Loading the pdfs and making chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/310.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -qq pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(\"/content/pdfs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files loaded:\n",
      "/content/pdfs/Data structures (Knapsack).pdf\n",
      "/content/pdfs/Data structures (Hasing).pdf\n",
      "/content/pdfs/Data structures (Graph).pdf\n",
      "/content/pdfs/Data structures (Binary Trees).pdf\n",
      "/content/pdfs/Data structures (Binary Search Tree).pdf\n",
      "/content/pdfs/Data structures (Linked List).pdf\n"
     ]
    }
   ],
   "source": [
    "files_loaded = set(doc.metadata['source'] for doc in data)\n",
    "print(\"Files loaded:\")\n",
    "for f in files_loaded:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "text_chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "177"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Microsoft® PowerPoint® 2016', 'creator': 'Microsoft® PowerPoint® 2016', 'creationdate': '2023-12-29T11:03:01+05:00', 'title': 'Slide 1', 'author': 'fsaleem', 'moddate': '2023-12-29T11:03:01+05:00', 'source': '/content/pdfs/Data structures (Binary Trees).pdf', 'total_pages': 41, 'page': 5, 'page_label': '6'}, page_content='Example')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFRrg6By1fFi"
   },
   "source": [
    "# **Generating the embeddings and storing it in FAAIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This is the old way of using hugging face wrapper, in newer version it is depreciated\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New way of using hugging face wrapper\n",
    "\n",
    "!pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -qq faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build FAISS vectorstore\n",
    "vectorstore = FAISS.from_documents(text_chunks, hf)\n",
    "vectorstore.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later if you want to load it\n",
    "# vectorstore = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ewpqsCm2LRl"
   },
   "source": [
    "# **Making a chat template of Interview QA for LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an interviewer.\n",
    "Generate {n} interview-style questions on the topic \"{topic}\"\n",
    "based only on the provided context.\n",
    "After each question, also provide a clear and correct answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Format:\n",
    "Q1: ...\n",
    "A1: ...\n",
    "Q2: ...\n",
    "A2: ...\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPCDWO7j2XM9"
   },
   "source": [
    "# **RAG chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from operator import itemgetter # when given a dict, returns the value for that key\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"topic\") | retriever,  # only pass \"topic\" to retriever\n",
    "        \"topic\": itemgetter(\"topic\"),\n",
    "        \"n\": itemgetter(\"n\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1:  Explain the difference between the 0/1 Knapsack problem and the Fractional Knapsack problem, and how this difference impacts the approach to solving them.\n",
      "\n",
      "A1: The 0/1 Knapsack problem deals with indivisible items; you can either take an entire item or none of it.  The Fractional Knapsack problem allows you to take fractions of items.  This key difference leads to different solution approaches. The 0/1 Knapsack problem is typically solved using dynamic programming because of the discrete nature of the items.  The Fractional Knapsack problem, on the other hand, can be efficiently solved using a greedy approach, prioritizing items with the highest value-to-weight ratio.\n",
      "\n",
      "\n",
      "Q2:  The provided text mentions that the 0/1 Knapsack problem can be solved using dynamic programming.  Describe at a high level how a dynamic programming solution would work.\n",
      "\n",
      "A2: A dynamic programming solution to the 0/1 Knapsack problem typically uses a table (often a 2D array) to store the maximum value achievable for a given weight capacity and a subset of items.  The table is filled iteratively, considering for each item whether including it (if the weight allows) increases the maximum value.  The final cell of the table represents the maximum value achievable with the given weight capacity and all items.  The process involves building up solutions to subproblems (smaller knapsack capacities and subsets of items) to solve the larger problem.\n",
      "\n",
      "\n",
      "Q3: Based on the problem description, what are the key inputs required to solve a Knapsack problem?\n",
      "\n",
      "A3: To solve a Knapsack problem, you need two key pieces of input: 1) The weight capacity of the knapsack, and 2) A list of items, each with its associated weight and value.\n"
     ]
    }
   ],
   "source": [
    "result = rag_chain.invoke({\n",
    "    \"topic\": \"How to solve Knapsack problem?\",\n",
    "    #\"topic\": \"Tell me about Riyan\",\n",
    "    \"n\": 3\n",
    "})\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
