{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrYO_haz8bIQ"
   },
   "source": [
    "# **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq langchain\n",
    "!pip install -qq langchain-community\n",
    "!pip install -qq langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_API_KEY'] = GEMINI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F85jo0rd8QBZ"
   },
   "source": [
    "# **1) Static prompts (plain templates, no variables)**\n",
    "\n",
    "Use when the instruction never changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **Improved Product Descriptions:** RAG can enhance product descriptions by incorporating information from various sources like specifications, reviews, and competitor offerings.\n",
      "\n",
      "2. **Personalized Recommendations:** RAG can generate more relevant product recommendations by accessing and synthesizing user data, purchase history, and product information.\n",
      "\n",
      "3. **Enhanced Customer Service:** RAG can power chatbots that provide more accurate and comprehensive answers to customer queries by accessing a knowledge base of product information and FAQs.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are concise and factual.\"),\n",
    "    (\"human\", \"List three practical uses of Retrieval-Augmented Generation for e-commerce.\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "print(chain.invoke({}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWAXOkya8gVt"
   },
   "source": [
    "# **2) Dynamic prompts (variables + partials)**\n",
    "\n",
    "1. Inject runtime values into the template; reuse with different inputs.\n",
    "2. Also handy: partials to “pre-fill” some variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stay perfectly hydrated with AquaSense!  This sleek stainless steel smart bottle boasts a built-in temperature display and smart hydration reminders, ensuring you drink enough throughout the day.  Never guess your water temperature again – AquaSense keeps you informed and refreshed.  Upgrade your hydration game today!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "base = PromptTemplate.from_template(\n",
    "    \"Write a {tone} {words}-word product blurb.\\n\"\n",
    "    \"Name: {name}\\n\"\n",
    "    \"Features: {features}\\n\"\n",
    ")\n",
    "\n",
    "# Pre-fill some variables once (partials can stack)\n",
    "prompt = base.partial(tone=\"engaging\").partial(words=\"60\")\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "print(chain.invoke({\n",
    "    \"name\": \"AquaSense Smart Bottle\",\n",
    "    \"features\": \"temperature display, hydration reminders, stainless steel\"\n",
    "}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHgxOv7I8xkE"
   },
   "source": [
    "# **3) Role-based (chat) prompts (system/human + message placeholders)**\n",
    "\n",
    "1. Use chat roles to separate “behavior” (system) from “request” (human).\n",
    "2. Add MessagesPlaceholder now and you can plug in memory later with the same prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```typescript\n",
      "type Shape = \n",
      "  | { kind: \"circle\"; radius: number }\n",
      "  | { kind: \"square\"; sideLength: number };\n",
      "\n",
      "function getArea(shape: Shape) {\n",
      "  switch (shape.kind) {\n",
      "    case \"circle\": return Math.PI * shape.radius ** 2;\n",
      "    case \"square\": return shape.sideLength ** 2;\n",
      "  }\n",
      "}\n",
      "\n",
      "```\n",
      "\n",
      "**Note:** The `kind` property acts as a discriminator, allowing TypeScript to narrow down the type based on its value, ensuring type safety within the `getArea` function.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a senior TypeScript mentor. Answer with precise code and a short note.\"),\n",
    "    MessagesPlaceholder(\"history\"),     # optional, for chat memory later\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# no history yet; just pass an empty list\n",
    "print(chain.invoke({\n",
    "    \"history\": [],\n",
    "    \"question\": \"Explain discriminated unions with a minimal example.\"\n",
    "}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2H_Q0pwv9Gvd"
   },
   "source": [
    "# **4) Few-shot prompts (teach by examples)**\n",
    "\n",
    "Prime the model with several (Q,A) examples before your real query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vxw71cMQ9Jfw"
   },
   "source": [
    "## **4a) Text few-shot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = πr²;  Area equals pi times the radius squared.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "examples = [\n",
    "    {\"question\": \"Convert 30°C to Fahrenheit.\", \"answer\": \"86°F (30×9/5 + 32).\"},\n",
    "    {\"question\": \"Plural of 'cactus'?\", \"answer\": \"'cacti' (also 'cactuses' in informal use).\"},\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\n",
    "    \"Q: {question}\\nA: {answer}\"\n",
    ")\n",
    "\n",
    "few_shot = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"You are a concise tutor. Use the examples to guide your answer.\\n\",\n",
    "    suffix=\"Q: {input}\\nA:\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "chain = few_shot | llm | StrOutputParser()\n",
    "print(chain.invoke({\"input\": \"Give the formula and one-line intuition for the area of a circle.\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlyp6NmH9dTU"
   },
   "source": [
    "## **4b) Chat few-shot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use a HashMap when you need fast lookups by key, and an array when you need fast access by index.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chat_example = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{question}\"),\n",
    "    (\"ai\", \"{answer}\")\n",
    "])\n",
    "\n",
    "# format examples into chat messages\n",
    "example_msgs = []\n",
    "for ex in examples:\n",
    "    example_msgs += chat_example.format_messages(**ex)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer succinctly and correctly.\"),\n",
    "    *example_msgs,\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "print(chain.invoke({\"input\": \"When should I use a hashmap vs an array?\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbP2WQ3E9r9L"
   },
   "source": [
    "# **format instructions pattern (for JSON outputs)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'LangChain', 'summary': 'LangChain is a framework for building applications powered by large language models (LLMs).  It simplifies development by providing tools for managing prompts, chains of operations, external tools, and memory.', 'tags': ['LLM', 'Prompt Engineering', 'Chain', 'Tools', 'Memory']}\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers.structured import StructuredOutputParser, ResponseSchema\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "schemas = [\n",
    "    ResponseSchema(name=\"title\", description=\"Short title\"),\n",
    "    ResponseSchema(name=\"summary\", description=\"2-3 sentence summary\"),\n",
    "    ResponseSchema(name=\"tags\", description=\"Array of 3-5 tags\"),\n",
    "]\n",
    "parser = StructuredOutputParser.from_response_schemas(schemas)\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Extract structured info from the text.\\n\"\n",
    "    \"{format_instructions}\\n\"\n",
    "    \"Text:\\n{text}\"\n",
    ").partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "print(chain.invoke({\"text\": \"LangChain lets you build LLM apps via prompts, chains, tools, and memory.\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
