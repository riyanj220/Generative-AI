{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOaSHkFv7C-6"
   },
   "source": [
    "# **Setting up API Keys**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "PINCECONE_API_KEY = userdata.get('PINCECONE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINCECONE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini API setup\n",
    "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
    "GEMINI_API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:embedContent\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-aFaWsDs7J0t"
   },
   "source": [
    "# **Installing and Importing libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq pinecone\n",
    "!pip install -q langchain langchain-community langchain-text-splitters requests\n",
    "!pip install -qq pypdf\n",
    "!pip install --upgrade langchain langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "import requests\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQ6cehsG7sq8"
   },
   "source": [
    "# **Raw pinecone client for debugging purposes ( we will use langchain for it)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 3072,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {},\n",
      " 'total_vector_count': 0,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "# Initialize client\n",
    "pc = Pinecone(api_key=PINCECONE_API_KEY)\n",
    "\n",
    "# Connect to your existing index\n",
    "index = pc.Index(\"test\")\n",
    "\n",
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ifTWXJB7yyT"
   },
   "source": [
    "# **Data loading and prepration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 1hPQlXrX8FbaYaLypxTmeVOFNitbBMlEE -O pdfs/yolov7paper.pdf\n",
    "!gdown 1vILwiv6nS2wI3chxNabMgry3qnV67TxM -O pdfs/rachelgreecv.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(\"pdfs\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "text_chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXMi10NZ8AqY"
   },
   "source": [
    "# **Gemini Embedding Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Gemini Embedding class\n",
    "class GeminiEmbedding(Embeddings):\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [self._embed(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return self._embed(text)\n",
    "\n",
    "    def _embed(self, text: str) -> List[float]:\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        params = {\"key\": self.api_key}\n",
    "        payload = {\"model\": \"models/gemini-embedding-001\", \"content\": {\"parts\":[{\"text\": text}]}}\n",
    "        response = requests.post(GEMINI_API_URL, json=payload, headers=headers, params=params)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Gemini API error: {response.text}\")\n",
    "\n",
    "        data = response.json()\n",
    "        return data[\"embedding\"][\"values\"]\n",
    "\n",
    "# Setup\n",
    "embedding = GeminiEmbedding(api_key=GEMINI_API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xS3v8ocY-Rir"
   },
   "source": [
    "# **Embedding and storing in pinecone using langchain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain-pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,     # your split PDF docs\n",
    "    embedding=embedding,\n",
    "    index_name=\"test\"          # same index you created\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLCzSFYw-ZYq"
   },
   "source": [
    "# **If you have done everything before and want to connect and query whenever, without re-embedding everything.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the index\n",
    "vectorstore = PineconeVectorStore.from_existing_index(\n",
    "    index_name=\"test\",\n",
    "    embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 3072,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'': {'vector_count': 168}},\n",
      " 'total_vector_count': 168,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlQW9msU665w"
   },
   "source": [
    "# **RAG pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity\n",
      "{'k': 3}\n"
     ]
    }
   ],
   "source": [
    "# 1. Create retriever from Pinecone\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "print(retriever.search_type)        # should be 'similarity'\n",
    "print(retriever.search_kwargs)      # {'k': 3}\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    google_api_key=GEMINI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Build RetrievalQA chain\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: YOLOv7 is a real-time object detector that surpasses other detectors in both speed and accuracy, achieving the highest accuracy (56.8% AP) among known detectors in the 5 FPS to 160 FPS range.  It was trained only on the MS COCO dataset without using other datasets or pre-trained weights.  Compared to YOLOv5-X, YOLOv7-X reduces parameters and computation while improving accuracy.\n",
      "\n",
      "Sources:\n",
      "- pdfs/yolov7paper.pdf\n",
      "- pdfs/yolov7paper.pdf\n",
      "- pdfs/yolov7paper.pdf\n"
     ]
    }
   ],
   "source": [
    "# 3. Ask a question\n",
    "\n",
    "query1 = \"What is YOLOv7 about?\"\n",
    "query2 = \"What is my name?\" # For testing like Gemini is actually citing the documents or not\n",
    "response = qa_chain.invoke({\"query\": query1})\n",
    "\n",
    "print(\"Answer:\", response[\"result\"])\n",
    "print(\"\\nSources:\")\n",
    "for doc in response[\"source_documents\"]:\n",
    "    print(\"-\", doc.metadata.get(\"source\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
