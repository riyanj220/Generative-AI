{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqBLMhaVpS3u"
   },
   "source": [
    "# **Setup & installs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.7/564.7 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade -q bitsandbytes transformers accelerate datasets peft trl huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboard==2.19.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, gc, locale\n",
    "# GC -> Garbage collector we use it when we want to free some memory while training models\n",
    "# locale -> Deals with language/encoding settings on your computer. Sometimes saving tokenizer/model files causes encoding errors. Forcing locale to \"UTF-8\" fixes that.\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer,\n",
    "    BitsAndBytesConfig, TrainingArguments, pipeline, logging\n",
    ")\n",
    "# BitsAndBytesConfig-> for quantization to load model in lower precision (4-bit or 8-bit) so it fits in GPU memory.\n",
    "\n",
    "\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "'''\n",
    "Trl: SFTTrainer\n",
    "“Supervised Fine-Tuning” trainer.\n",
    "Simplifies fine-tuning language models on instruction datasets.\n",
    "It uses your dataset’s text field and trains the model with LoRA efficiently.\n",
    "'''\n",
    "\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8562cd967af4d3daa35ce7f5120b6fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (Optional) login now to avoid later prompts\n",
    "login()  # paste your HF token (needs access to Llama 2 gated model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dtype: torch.float16\n"
     ]
    }
   ],
   "source": [
    "# Make sure UTF-8 (avoids tokenizer save issues in some environments)\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "# Pick device dtype\n",
    "USE_BF16 = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8  # A100 or better\n",
    "DTYPE = torch.bfloat16 if USE_BF16 else torch.float16\n",
    "print(\"Using dtype:\", DTYPE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIFxPuwapbJZ"
   },
   "source": [
    "# **Config: model, dataset, QLoRA, LoRA, training args**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME   = \"meta-llama/Llama-2-7b-chat-hf\"   # ensure HF access approved\n",
    "DATASET_NAME = \"mlabonne/guanaco-llama2-1k\"        # has a 'text' field\n",
    "NEW_MODEL_ID = \"Riyan213/llama2-7b-chat-qlora-demo\"  # where you’ll push\n",
    "\n",
    "# QLoRA (4-bit) quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=DTYPE,   # bf16 on A100, else fp16\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Training args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=25,\n",
    "    save_strategy=\"epoch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    fp16=not USE_BF16,\n",
    "    bf16=USE_BF16,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxqPrHSMpggo"
   },
   "source": [
    "# **Load dataset, tokenizer, base model (4-bit)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_NAME, split=\"train\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clIDqEpbpmOw"
   },
   "source": [
    "# **Trainer (TRL SFTTrainer) & training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da99ffbf53f448e9ab9c6a09ccfb2e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23afac94a7754b9a803daf8b33333981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5dce00fff214845b77ba0eb3b6daf81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28da8383e760461cbc9bfa440bec7c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 18:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.588600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.323400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.271900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.327700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.217600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.212400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.249600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.271000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.209800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.185600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter saved to: ./adapter-qlora\n"
     ]
    }
   ],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    return example[\"text\"]\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    args=training_args,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save adapter weights + tokenizer locally\n",
    "ADAPTER_DIR = \"./adapter-qlora\"\n",
    "trainer.model.save_pretrained(ADAPTER_DIR)\n",
    "tokenizer.save_pretrained(ADAPTER_DIR)\n",
    "print(\"Adapter saved to:\", ADAPTER_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cym-jqxwpqpQ"
   },
   "source": [
    "# **Monitor with TensorBoard**\n",
    "---\n",
    "TensorBoard = a dashboard for training deep learning models.\n",
    "\n",
    "It shows live graphs and charts of how your model is doing during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# in Colab:\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ql3MaDb7pvVt"
   },
   "source": [
    "# **Quick test generation (with adapters still attached)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] What is a large language model? [/INST] A large language model is a machine learning model that is trained on a large dataset of text, and is capable of generating text that is similar to human language. It is typically trained using a variant of the transformer architecture, and is trained on large datasets such as the internet or a large corpus of text. Large language models are capable of generating text that is coherent and contextually appropriate, and can be used for a wide range of applications such as language translation, text summarization, and chatbots. They are also often used for research purposes, such as studying the properties of language and understanding how to improve machine learning models. \n"
     ]
    }
   ],
   "source": [
    "gen = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=trainer.model,\n",
    "    tokenizer=tokenizer,\n",
    "    dtype=DTYPE,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=200,\n",
    ")\n",
    "print(gen(\"[INST] What is a large language model? [/INST]\")[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0MtmbhUp0U-"
   },
   "source": [
    "# **Free VRAM (cleanly) before merging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del gen\n",
    "del trainer\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmEx8xDpp4EE"
   },
   "source": [
    "# **Merge LoRA adapters into full FP16 model (for easy deployment)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be023ab1659401caab65d6e5aab7324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    dtype=DTYPE,   # bf16/fp16 full model (no 4-bit here)\n",
    ")\n",
    "\n",
    "merged = PeftModel.from_pretrained(base_model, ADAPTER_DIR)\n",
    "merged = merged.merge_and_unload()  # weights merged into base_model\n",
    "\n",
    "# Save merged model + tokenizer\n",
    "MERGED_DIR = \"./merged-fp16\"\n",
    "merged.save_pretrained(MERGED_DIR)\n",
    "tokenizer.save_pretrained(MERGED_DIR)\n",
    "print(\"Merged model saved to:\", MERGED_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kii2bKBfp8Mm"
   },
   "source": [
    "# **(Optional) Quick test on merged model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=merged,\n",
    "    tokenizer=tokenizer,\n",
    "    dtype=DTYPE,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=200,\n",
    ")\n",
    "print(merged_pipe(\"[INST] Explain QLoRA in simple terms. [/INST]\")[0][\"generated_text\"])\n",
    "\n",
    "# cleanup\n",
    "del merged_pipe\n",
    "del merged\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKXLH4Z9p_rJ"
   },
   "source": [
    "# **Push to Hugging Face Hub (adapters or merged)**\n",
    "\n",
    "Push only LoRA adapters (lightweight, preferred for sharing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import create_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADAPTER_REPO_ID = NEW_MODEL_ID + \"-adapters\"\n",
    "create_repo(ADAPTER_REPO_ID, exist_ok=True)\n",
    "from peft import PeftModel\n",
    "# reload adapter weights dir & push\n",
    "from transformers import AutoModelForCausalLM\n",
    "base = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"cpu\")\n",
    "peft_model = PeftModel.from_pretrained(base, ADAPTER_DIR)\n",
    "peft_model.push_to_hub(ADAPTER_REPO_ID)\n",
    "tokenizer.push_to_hub(ADAPTER_REPO_ID)\n",
    "print(\"Adapters pushed to:\", ADAPTER_REPO_ID)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
